{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explainability scores\n",
    "\n",
    "This notebook is used to get explainability attributions for previously trained models. It is using the same configuration as the [starter](starter.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model\n",
    "\n",
    "You can load your already processed data with `load_data` and the `from_cleaned` argument (be sure to have specified the path for the cleaned data in the config file). The additional arguments need to be the same as the ones used for training, but if `from_cleaned` is `True`, they will be `None` by default.\n",
    "\n",
    "You can load your trained model with `load_model` method with `.pt` file path as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from beexai.dataset.dataset import Dataset\n",
    "from beexai.dataset.load_data import load_data\n",
    "from beexai.explanation.explaining import CaptumExplainer\n",
    "\n",
    "from beexai.explanation.plot_attr import bar_plot, plot_swarm, plot_waterfall\n",
    "from beexai.training.train import Trainer\n",
    "from beexai.utils.time_seed import set_seed\n",
    "from beexai.utils.sampling import stratified_sampling\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_NAME = \"kickstarter\"\n",
    "MODEL_NAME = \"NeuralNetwork\"\n",
    "NUM_SAMPLES = 100\n",
    "data_test, target_col, task, _ = load_data(\n",
    "    from_cleaned=True, config_path=f\"config/{DATA_NAME}.yml\"\n",
    ")\n",
    "scale_params = {\n",
    "    \"x_num_scaler_name\": \"quantile_normal\",\n",
    "    \"x_cat_encoder_name\": \"ordinalencoder\",\n",
    "    \"y_scaler_name\": \"labelencoder\",  # change to minmax or another float scaler for regression\n",
    "    \"cat_not_to_onehot\": [\"name\"],\n",
    "}\n",
    "data = Dataset(data_test, target_col)\n",
    "X_train, X_test, y_train, y_test = data.get_train_test(\n",
    "    test_size=0.2, scaler_params=scale_params\n",
    ")\n",
    "\n",
    "NUM_LABELS = data.get_classes_num(task)\n",
    "NN_PARAMS = {\"input_dim\": X_train.shape[1], \"output_dim\": NUM_LABELS}\n",
    "trainer = Trainer(MODEL_NAME, task, NN_PARAMS, device)\n",
    "trainer.load_model(f\"../output/models/{DATA_NAME}/{MODEL_NAME}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster testing, we use the function `stratified_sampling` that samples a fraction of the data while keeping the same distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = stratified_sampling(X_test, y_test, NUM_SAMPLES, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get attributions\n",
    "\n",
    "Explanation methods are based on Captum library including `ShapleyValueSampling`, `FeatureAblation`, `KernelSHAP`, `IntegratedGradients`, `InputXGradient`, `Saliency` and `DeepLift`. \n",
    "\n",
    "Be aware that Gradient based methods can't work with non Deep Learning models.\n",
    "\n",
    "The method `compute_attributions` automatically computes the attributions for each sample in the test set and save them in the specified folder. If attributions are already computed in the folder, it will automatically load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"IntegratedGradients\"\n",
    "explainer = CaptumExplainer(\n",
    "    trainer.model, task=task, method=\"IntegratedGradients\", sklearn=False, device=device\n",
    ")\n",
    "\n",
    "explainer.init_explainer()\n",
    "all_preds = trainer.model.predict(X_test.values)\n",
    "attributions = explainer.compute_attributions(\n",
    "    X_test,\n",
    "    DATA_NAME,\n",
    "    MODEL_NAME,\n",
    "    METHOD,\n",
    "    \"../output/explain/\",\n",
    "    all_preds,\n",
    "    save=True,\n",
    "    use_abs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different plots similar to the ones provided by SHAP can be obtained for any kind of attributions. Be sure that the features names provided are in the same order as the ones used for training:\n",
    "- `bar_plot` directly plots the attributions for a specific instance or the mean of the attributions for all instances in an histogram.\n",
    "- `plot_waterfall` plots the attributions for a specific instance in a waterfall plot.\n",
    "- `plot_swarm` plots the attributions for all instances in a swarm plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = list(data_test.columns)\n",
    "features_names.remove(target_col)\n",
    "bar_plot(attributions, feature_names=features_names, mean=True)\n",
    "plot_waterfall(attributions[0], feature_names=features_names, mean=False)\n",
    "plot_swarm(X_test, attributions, feature_names=features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Go to [Metric](metrics.ipynb) to get the metrics on the attributions you just computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
