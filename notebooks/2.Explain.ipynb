{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explainability scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model\n",
    "\n",
    "You can load your already processed data with `load_data` and the `from_cleaned` argument (be sure to have specified the path for the cleaned data in the config file). The additional arguments need to be the same as the ones used for training, but if `from_cleaned` is `True`, they will be `None` by default. The same scaling used during training needs to be applied too.\n",
    "\n",
    "You can load your trained model with `load_model` method of `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from beexai.dataset.dataset import Dataset\n",
    "from beexai.dataset.load_data import load_data\n",
    "from beexai.explanation.explaining import CaptumExplainer\n",
    "\n",
    "from beexai.explanation.plot_attr import bar_plot, plot_swarm, plot_waterfall\n",
    "from beexai.training.train import Trainer\n",
    "from beexai.utils.time_seed import set_seed\n",
    "from beexai.utils.sampling import stratified_sampling\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_NAME = \"kickstarter\"\n",
    "MODEL_NAME = \"NeuralNetwork\"\n",
    "NUM_SAMPLES = 100\n",
    "data_test, target_col, task, _ = load_data(\n",
    "    from_cleaned=True, config_path=f\"config/{DATA_NAME}.yml\"\n",
    ")\n",
    "scale_params = {\n",
    "    \"x_num_scaler_name\": \"quantile_normal\",\n",
    "    \"x_cat_encoder_name\": \"ordinalencoder\",\n",
    "    \"y_scaler_name\": \"labelencoder\",  # change to minmax or another float scaler for regression\n",
    "    \"cat_not_to_onehot\": [\"name\"],\n",
    "}\n",
    "data = Dataset(data_test, target_col)\n",
    "X_train, X_test, y_train, y_test = data.get_train_test(\n",
    "    test_size=0.2, scaler_params=scale_params\n",
    ")\n",
    "\n",
    "NUM_LABELS = data.get_classes_num(task)\n",
    "NN_PARAMS = {\"input_dim\": X_train.shape[1], \"output_dim\": NUM_LABELS}\n",
    "trainer = Trainer(MODEL_NAME, task, NN_PARAMS, device)\n",
    "# trainer = Trainer(\"XGBClassifier\", task, device=device)\n",
    "trainer.load_model(f\"../output/models/{DATA_NAME}/{MODEL_NAME}.pt\")\n",
    "\n",
    "X_test, y_test = stratified_sampling(X_test, y_test, NUM_SAMPLES, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get attributions\n",
    "\n",
    "Explanation methods are based on Captum library including ShapleyValueSampling, FeatureAblation, KernelSHAP, IntegratedGradients, InputXGradient, Saliency, DeepLift. You can choose to work with a sklearn model that will have a specific wrapper or just a torch model.\n",
    "Be aware that Gradient based methods can't work with non Deep Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Test Captum####\n",
    "METHOD = \"IntegratedGradients\"  # change this to a Non-Gradient based method for sklearn models\n",
    "explainer = CaptumExplainer(\n",
    "    trainer.model, task=task, method=\"IntegratedGradients\", sklearn=False, device=device\n",
    ")  # for sklearn models, change sklearn=False to True\n",
    "\n",
    "explainer.init_explainer()\n",
    "all_preds = trainer.model.predict(X_test.values)\n",
    "attributions = explainer.compute_attributions(\n",
    "    X_test,\n",
    "    DATA_NAME,\n",
    "    MODEL_NAME,\n",
    "    METHOD,\n",
    "    \"../output/explain/\",\n",
    "    all_preds,\n",
    "    save=True,\n",
    "    use_abs=False,  # change this to True for regression (more details in the paper)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attributions values, waterfall and swarm plot\n",
    "\n",
    "features_names = list(data_test.columns)\n",
    "features_names.remove(target_col)\n",
    "bar_plot(attributions, feature_names=features_names, mean=True)\n",
    "plot_waterfall(attributions[0], feature_names=features_names, mean=False)\n",
    "plot_swarm(X_test, attributions, feature_names=features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Go to `3.Metrics.ipynb` to get the metrics on the attributions you just computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
